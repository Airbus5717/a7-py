"""
Markdown documentation formatter for A7 compiler.

Generates a complete markdown document showing the full compilation pipeline:
source code, tokens, AST, semantic analysis, and generated output.
"""

from datetime import datetime
from typing import Optional, List, Dict


class MarkdownFormatter:
    """Generates markdown documentation of a complete compilation."""

    def format_compilation_doc(
        self,
        input_path: str,
        source_code: str,
        tokens: list,
        ast,
        semantic_results: Optional[Dict],
        codegen_result: Optional[Dict],
    ) -> str:
        """Generate full compilation documentation as markdown."""
        lines = []

        # Header
        lines.append(f"# Compilation Report: `{input_path}`")
        lines.append("")
        lines.append(f"Generated by A7 compiler on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Table of contents
        lines.append("## Table of Contents")
        lines.append("")
        lines.append("1. [Source Code](#1-source-code)")
        lines.append("2. [Stage 1: Lexical Analysis](#2-stage-1-lexical-analysis)")
        lines.append("3. [Stage 2: Syntactic Analysis](#3-stage-2-syntactic-analysis)")
        lines.append("4. [Stage 3: Semantic Analysis](#4-stage-3-semantic-analysis)")
        lines.append("5. [Stage 4: Code Generation](#5-stage-4-code-generation)")
        lines.append("6. [Summary](#6-summary)")
        lines.append("")
        lines.append("---")
        lines.append("")

        # 1. Source Code
        lines.append("## 1. Source Code")
        lines.append("")
        lines.append(f"**File:** `{input_path}`  ")
        lines.append(f"**Lines:** {len(source_code.splitlines())}  ")
        lines.append(f"**Size:** {len(source_code)} bytes")
        lines.append("")
        lines.append("```")
        lines.append(source_code.rstrip())
        lines.append("```")
        lines.append("")

        # 2. Lexical Analysis
        lines.append("## 2. Stage 1: Lexical Analysis")
        lines.append("")
        real_tokens = [t for t in tokens if t.type.name != 'EOF']
        lines.append(f"The tokenizer produced **{len(real_tokens)} tokens** from the source code.")
        lines.append("")
        lines.append("| # | Line:Col | Token Type | Value | Length |")
        lines.append("|---|----------|------------|-------|--------|")
        for i, token in enumerate(real_tokens):
            val = repr(token.value).replace("|", "\\|") if token.value else "''"
            ttype = token.type.name
            length = str(token.length) if hasattr(token, "length") else "?"
            lines.append(f"| {i} | {token.line}:{token.column} | `{ttype}` | {val} | {length} |")
        lines.append("")

        # 3. Syntactic Analysis
        lines.append("## 3. Stage 2: Syntactic Analysis")
        lines.append("")
        if ast:
            decl_count = len(ast.declarations) if hasattr(ast, 'declarations') and ast.declarations else 0
            lines.append(f"The parser produced an AST with **{decl_count} top-level declaration(s)**.")
            lines.append("")
            lines.append("### AST Structure")
            lines.append("")
            lines.append("```")
            self._format_ast_tree(ast, lines, indent=0)
            lines.append("```")
            lines.append("")
        else:
            lines.append("*Parsing failed or was skipped.*")
            lines.append("")

        # 4. Semantic Analysis
        lines.append("## 4. Stage 3: Semantic Analysis")
        lines.append("")
        if semantic_results:
            passes = semantic_results.get("passes", [])
            errors = semantic_results.get("errors", [])
            symbol_table = semantic_results.get("symbol_table")

            # Pass results
            lines.append("### Analysis Passes")
            lines.append("")
            lines.append("| Pass | Status | Errors |")
            lines.append("|------|--------|--------|")
            for p in passes:
                name = p.get("name", "Unknown")
                ok = p.get("ok", False)
                err_count = p.get("errors", 0)
                status = "Pass" if ok else "**Fail**"
                lines.append(f"| {name} | {status} | {err_count} |")
            lines.append("")

            # Symbol table
            if symbol_table:
                symbols = self._collect_symbols(symbol_table)
                if symbols:
                    lines.append("### Symbol Table")
                    lines.append("")
                    lines.append(f"Found **{len(symbols)} symbols** in the program.")
                    lines.append("")
                    lines.append("| Name | Kind | Type | Scope |")
                    lines.append("|------|------|------|-------|")
                    for sym in symbols:
                        lines.append(f"| `{sym['name']}` | {sym['kind']} | `{sym['type']}` | {sym['scope']} |")
                    lines.append("")

            # Errors
            if errors:
                lines.append("### Semantic Errors")
                lines.append("")
                for err in errors:
                    msg = str(err) if not hasattr(err, 'message') else err.message
                    line_num = ""
                    if hasattr(err, 'span') and err.span:
                        line_num = f" (line {err.span.start_line})"
                    lines.append(f"- {msg}{line_num}")
                lines.append("")
        else:
            lines.append("*Semantic analysis was skipped.*")
            lines.append("")

        # 5. Code Generation
        lines.append("## 5. Stage 4: Code Generation")
        lines.append("")
        if codegen_result:
            output_code = codegen_result.get("output_code", "")
            output_path = codegen_result.get("output_path", "")
            byte_count = codegen_result.get("bytes", 0)

            lines.append(f"**Backend:** Zig  ")
            lines.append(f"**Output:** `{output_path}`  ")
            lines.append(f"**Size:** {byte_count} bytes")
            lines.append("")
            lines.append("```zig")
            lines.append(output_code.rstrip())
            lines.append("```")
            lines.append("")
        else:
            lines.append("*Code generation was skipped or failed.*")
            lines.append("")

        # 6. Summary
        lines.append("## 6. Summary")
        lines.append("")
        token_count = len(real_tokens)
        decl_count = len(ast.declarations) if ast and hasattr(ast, 'declarations') and ast.declarations else 0
        error_count = len(semantic_results.get("errors", [])) if semantic_results else 0
        output_bytes = codegen_result.get("bytes", 0) if codegen_result else 0

        lines.append("| Stage | Result |")
        lines.append("|-------|--------|")
        lines.append(f"| Lexer | {token_count} tokens |")
        lines.append(f"| Parser | {decl_count} declarations |")
        if error_count > 0:
            lines.append(f"| Semantic | {error_count} error(s) |")
        else:
            lines.append(f"| Semantic | Clean |")
        if output_bytes > 0:
            lines.append(f"| Codegen | {output_bytes} bytes |")
        else:
            lines.append(f"| Codegen | Skipped |")
        lines.append("")

        return "\n".join(lines)

    def _format_ast_tree(self, node, lines: list, indent: int = 0) -> None:
        """Format AST as indented text tree (iterative)."""
        if node is None:
            return

        # Stack of (node, indent_level)
        stack = [(node, indent)]
        while stack:
            current, cur_indent = stack.pop()
            if current is None:
                continue

            prefix = "  " * cur_indent
            kind = current.kind.name if hasattr(current, 'kind') else "?"

            label = kind
            if hasattr(current, 'name') and current.name:
                label += f" '{current.name}'"
            if hasattr(current, 'literal_kind') and current.literal_kind:
                label += f" ({current.literal_kind.name})"
                val = getattr(current, 'literal_value', None)
                if val is not None:
                    label += f" = {val}"
            if hasattr(current, 'operator') and current.operator:
                label += f" [{current.operator.name}]"

            lines.append(f"{prefix}{label}")

            # Collect children in order, then push reversed so first child is processed first
            children = []
            if hasattr(current, 'declarations') and current.declarations:
                children.extend(current.declarations)
            if hasattr(current, 'parameters') and current.parameters:
                children.extend(current.parameters)
            if hasattr(current, 'body') and current.body:
                children.append(current.body)
            if hasattr(current, 'statements') and current.statements:
                children.extend(current.statements)
            if hasattr(current, 'fields') and current.fields:
                children.extend(current.fields)
            if hasattr(current, 'variants') and current.variants:
                children.extend(current.variants)

            for child in reversed(children):
                stack.append((child, cur_indent + 1))

    def _collect_symbols(self, symbol_table) -> list:
        """Collect symbols from symbol table."""
        symbols = []
        try:
            scope = symbol_table.current_scope if hasattr(symbol_table, 'current_scope') else None
            if scope is None and hasattr(symbol_table, 'global_scope'):
                scope = symbol_table.global_scope
            visited = set()
            self._walk_scope(scope, symbols, "global", visited)
        except Exception:
            pass
        return symbols

    def _walk_scope(self, scope, symbols: list, scope_name: str, visited: set):
        """Walk scopes to collect symbols (iterative)."""
        if scope is None:
            return

        stack = [(scope, scope_name)]
        while stack:
            current, cur_name = stack.pop()
            if current is None or id(current) in visited:
                continue
            visited.add(id(current))

            sym_dict = getattr(current, 'symbols', {})
            for name, sym in sym_dict.items():
                kind_str = sym.kind.name if hasattr(sym, 'kind') and hasattr(sym.kind, 'name') else "?"
                type_str = str(sym.type) if hasattr(sym, 'type') and sym.type else "?"
                symbols.append({"name": name, "kind": kind_str, "type": type_str, "scope": cur_name})

            children = getattr(current, 'children', [])
            for i, child in enumerate(reversed(children)):
                child_name = getattr(child, 'name', f"scope_{len(children) - 1 - i}")
                stack.append((child, child_name))
